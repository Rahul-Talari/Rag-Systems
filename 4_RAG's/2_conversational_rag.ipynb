{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Indexing Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 1. Load the webpage and extract the relevant content (post title, headers, content)\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "docs = WebBaseLoader(web_paths=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\"], bs_kwargs={\"parse_only\": bs4_strainer}).load()\n",
    "\n",
    "# 2. Chunk the text for embedding\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "page_content = [doc.page_content for doc in docs]\n",
    "chunks = []\n",
    "for content in page_content:\n",
    "    chunks.extend(recursive_text_splitter.split_text(content))\n",
    "\n",
    "# 3. Embed the content and store it in FAISS vector store\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = embed.embed_documents(chunks)  # Generate embeddings\n",
    "vectorstore = FAISS.from_embeddings(list(zip(chunks, embeddings)), embedding=embed)  # Save embeddings to FAISS vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversational RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create History-Aware Retriever Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Define the contextualize question prompt\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", contextualize_q_system_prompt),\n",
    "     MessagesPlaceholder(\"chat_history\"),\n",
    "     (\"human\", \"{input}\")]\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.8)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm,retriever, contextualize_q_prompt) # Create history-aware retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create Document Chain to Process Retrieved Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load LLM model\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.8)\n",
    "\n",
    "# Define the prompt for QA\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", qa_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\")]\n",
    ")\n",
    "\n",
    "# Create the document processing chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Combine Retrieval and Document Processing Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Stateful Chat History Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# State management for chat history\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap the retrieval and processing chain with chat history\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Generation: Final Combined Chain with Chat History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "An **Agent** is an autonomous entity that perceives its environment and takes actions to achieve specific goals. The term \"agent\" was introduced by Richard E. Markov and Åke Madsen in their 1963 paper \"On the Solution of Partially Observable Markov Decision Processes.\" An agent operates within a dynamic environment, where it can sense the current state (perception), select an action (action selection), and execute that action (行动执行) to influence the environment. The goal is typically to maximize some cumulative reward or achieve a desired outcome.\n",
      "\n",
      "### Key Components of an Agent:\n",
      "1. **Agent State**: Represents the current situation in the environment.\n",
      "2. **Action Space**: The set of possible actions the agent can take at any given state.\n",
      "3. **Perception**: The information the agent has about its environment, often through sensors or other means.\n",
      "4. **Reward Signal**: Provides feedback on the outcome of an action (e.g., positive or negative).\n",
      "5. **Policy**: A strategy that determines which action to select based on the current state.\n",
      "6. **Value Function**: Estimates the long-term reward associated with being in a particular state or taking a specific action.\n",
      "\n",
      "### Types of Agents:\n",
      "1. **Reinforcement Learning Agents**: Learn through trial and error without prior knowledge, adjusting actions based on feedback (rewards) received.\n",
      "2. **Model-Based Agents**: Precompute solutions to environmental dynamics, allowing for more predictable and stable behavior.\n",
      "3. **Model-Free Agents**: Do not precompute solutions; instead, they learn through interaction with the environment.\n",
      "\n",
      "### Example:机器人：\n",
      "A robot is an example of an agent. It perceives its environment (e.g., visual sensors), can select actions like moving left or right, executes these actions to manipulate the environment, and receives rewards based on how successful the action was.\n",
      "\n",
      "Agent design focuses on creating agents that are:\n",
      "- **Autonomous**: Irrespective of external influences.\n",
      "- **Adaptive**: Changes behavior based on changing environments.\n",
      "- **Generalizable**: Can operate in a wide variety of tasks across different domains.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "session_id = str(uuid.uuid4())  # Generates a unique identifier\n",
    "response=conversational_rag_chain.invoke({\"input\": \"What is Agent?\"},config={\"configurable\": {\"session_id\": session_id}})[\"answer\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Chat_history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b84b8f6e-0a1c-46cf-8d10-595afc88b0a2': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='<think>\\n\\n</think>\\n\\nTask decomposition refers to the process of breaking down a complex task into smaller, more manageable subtasks or components. This approach allows individuals or systems to tackle each part separately, reducing overwhelm and improving efficiency.\\n\\n### Key Aspects of Task Decomposition:\\n\\n1. **Simplification**: By dividing tasks, complexity is reduced, making them easier to understand and execute.\\n2. **Improved Management**: Subtasks can be prioritized and scheduled more effectively, ensuring clarity on priorities.\\n3. **Enhanced Retention**: Breakdown helps in retaining better skills by focusing on specific aspects before integrating them into a larger system.\\n4. **Enhanced Creativity**: Breaking tasks into smaller pieces can foster creativity as solutions often emerge when approached incrementally.\\n5. **Adaptability**: It makes the task more flexible, allowing adjustments to be made without losing focus.\\n\\n### Example:\\n\\nA large software development project can be decomposed into smaller phases: requirements gathering, design, implementation, testing, and deployment. Each phase becomes a separate task that needs to be completed before moving on to the next.\\n\\nTask decomposition is widely used in software development, Agile methodologies, and process improvement initiatives to manage complexity and achieve better outcomes.', additional_kwargs={}, response_metadata={})]), 'a7ee055f-489f-40b3-a315-ff2bcd6c4192': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Agent?', additional_kwargs={}, response_metadata={}), AIMessage(content='<think>\\n\\n</think>\\n\\nAn **Agent** is an autonomous entity that perceives its environment and takes actions to achieve specific goals. The term \"agent\" was introduced by Richard E. Markov and Åke Madsen in their 1963 paper \"On the Solution of Partially Observable Markov Decision Processes.\" An agent operates within a dynamic environment, where it can sense the current state (perception), select an action (action selection), and execute that action (行动执行) to influence the environment. The goal is typically to maximize some cumulative reward or achieve a desired outcome.\\n\\n### Key Components of an Agent:\\n1. **Agent State**: Represents the current situation in the environment.\\n2. **Action Space**: The set of possible actions the agent can take at any given state.\\n3. **Perception**: The information the agent has about its environment, often through sensors or other means.\\n4. **Reward Signal**: Provides feedback on the outcome of an action (e.g., positive or negative).\\n5. **Policy**: A strategy that determines which action to select based on the current state.\\n6. **Value Function**: Estimates the long-term reward associated with being in a particular state or taking a specific action.\\n\\n### Types of Agents:\\n1. **Reinforcement Learning Agents**: Learn through trial and error without prior knowledge, adjusting actions based on feedback (rewards) received.\\n2. **Model-Based Agents**: Precompute solutions to environmental dynamics, allowing for more predictable and stable behavior.\\n3. **Model-Free Agents**: Do not precompute solutions; instead, they learn through interaction with the environment.\\n\\n### Example:机器人：\\nA robot is an example of an agent. It perceives its environment (e.g., visual sensors), can select actions like moving left or right, executes these actions to manipulate the environment, and receives rewards based on how successful the action was.\\n\\nAgent design focuses on creating agents that are:\\n- **Autonomous**: Irrespective of external influences.\\n- **Adaptive**: Changes behavior based on changing environments.\\n- **Generalizable**: Can operate in a wide variety of tasks across different domains.', additional_kwargs={}, response_metadata={})])}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
