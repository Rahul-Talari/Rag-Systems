{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexing Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load, Chunk, Embed, Store\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Load documents from a URL\n",
    "docs = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\").load()  \n",
    "\n",
    "# Split documents into chunks\n",
    "chunk = RecursiveCharacterTextSplitter()\n",
    "documents = chunk.split_documents(docs)              \n",
    "document_texts = [doc.page_content for doc in documents]  # Extract text\n",
    "\n",
    "# Embed document texts\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = embed.embed_documents(document_texts)  # Generate embeddings\n",
    "\n",
    "# Create and save FAISS vector store\n",
    "vector = FAISS.from_embeddings(list(zip(document_texts, embeddings)), embedding=embed)\n",
    "vector.save_local(\"C:/Users/admin/OneDrive/Desktop/RAG'S/vector db/Faiss-index/\")  # Store locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval & Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Step 1: Setup retriever and embeddings\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vector = FAISS.load_local(\"C:/Users/admin/OneDrive/Desktop/RAG'S/vector db/Faiss-index/\", embeddings=embed, allow_dangerous_deserialization=True)\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# Initialize LLM\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "llm_mixtral = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Step 2: Contextualize Userâ€™s Question (History-Aware Retriever Chain)\n",
    "retrieval_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Generate a search query to look up relevant information\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm_mixtral, retriever, retrieval_prompt)\n",
    "\n",
    "# Step 3: Document Chain to Process Retrieved Context (Answer Generation)\n",
    "response_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's query based on the following retrieved context:\\n\\n{context}\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm_mixtral, response_prompt)\n",
    "\n",
    "# Step 4: Combine the Retrieval and Document Processing Chains into a single retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Function to process queries and update chat history\n",
    "def ask_question(query):  \n",
    "    global chat_history  \n",
    "    response = retrieval_chain.invoke({\"chat_history\": chat_history, \"input\": query})\n",
    "    answer = response['answer']\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    chat_history.append(AIMessage(content=answer))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply: Application development is the process of creating, designing, and implementing software applications for various devices and platforms. It involves various stages such as prototyping, beta testing, and production, where developers use tools and platforms like LangSmith to monitor, test, and improve the performance of their applications. LangSmith provides features such as tracing, evaluation, production monitoring, and automations to support developers in the application development lifecycle.\n"
     ]
    }
   ],
   "source": [
    "user_input=ask_question(\"what is application development?\")\n",
    "print(\"Reply:\", user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply: Based on the provided context, LangSmith is a platform for LLM (large language model) application development, monitoring, and testing. In the Beta Testing phase, it's important to collect human feedback on the application's responses, annotate traces, and add runs to datasets for refining and improving performance. In the Production phase, closely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are essential workflows.\n",
      "\n",
      "Monitoring charts in LangSmith allow tracking of key metrics over time, and tag and metadata grouping help users mark different versions of their applications and view how they perform side-by-side within each chart, which is useful for A/B testing changes in prompt, model, or retrieval strategy.\n",
      "\n",
      "Automations allow users to perform actions on traces in near real-time, and threads group traces from a single conversation together, making it easier to track the performance of and annotate applications across multiple turns.\n",
      "\n",
      "The provided context discusses various stages of LLM application development and how LangSmith can be utilized in each stage. However, the user's query is not directly addressed in the provided context.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Chatbot loop\n",
    "user_input = input(\"Enter something (type 'exit' to quit): \")\n",
    "\n",
    "while user_input != 'exit':  # Loop until 'exit' is typed\n",
    "    print(\"Reply:\", ask_question(user_input))  # Process user query and show AI's reply\n",
    "    user_input = input(\"Enter something (type 'exit' to quit): \")\n",
    "\n",
    "print(\"Goodbye!\")  # End the conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
