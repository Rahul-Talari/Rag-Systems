{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\u001b[?25lpulling manifest ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling aabd4debf0c8... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 GB                         \n",
      "pulling 369ca498f347... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  387 B                         \n",
      "pulling 6e4c38e1172f... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  148 B                         \n",
      "pulling a85fe2a2e58e... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU langchain-community langchain-ollama\n",
    "! ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM'S Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the LLM model\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invoke Model and Track Token Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? üòä\n",
      "\n",
      "Token Usage:\n",
      "Total Tokens: input tokens + output tokens = 20\n",
      "Prompt Tokens: tokens in the input = 4\n",
      "Completion Tokens: tokens in the output = 16\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "# Take input from the user\n",
    "user_query = input(\"Enter your question: \")\n",
    "\n",
    "# Track token usage with OpenAI callback\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.invoke(user_query)\n",
    "    \n",
    "    # Print response content\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response.content)\n",
    "    \n",
    "    # Print token usage details\n",
    "    print(\"\\nToken Usage:\")\n",
    "    print(f\"Total Tokens: input tokens + output tokens = {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: tokens in the input = {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: tokens in the output = {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agentic LLM'S Tracing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU langchain langchain_ollama tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "search_tool = TavilySearchResults()\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "agent = initialize_agent(tools=[search_tool], llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Alright, so I need to figure out Ramcharan's current age in 2025. Hmm, where do I start? Well, first things first, I remember that each person has a birthdate and their current age depends on how far along we are from that date. But wait, do I have any information about Ramcharan besides this question?\n",
      "\n",
      "Let me think... The user only provided the question about Ramcharan's age in 2025, so there must be some external data or context needed here. Maybe Ramcharan is a person who we're supposed to know by their public records? I'm not sure how accurate that would be, though.\n",
      "\n",
      "I could check online sources for public figures to see if anyone named Ramcharan exists, but that seems unlikely since Ramcharan doesn't ring any bells. Perhaps Ramcharan refers to an author or a fictional character? But then the question wouldn't make much sense without additional context.\n",
      "\n",
      "Wait, maybe there's another angle. Could this be related to a specific event or historical figure where Ramcharan is associated with a particular year and age? Without more information, it's hard to pinpoint their exact age in 2025.\n",
      "\n",
      "I should also consider that if Ramcharan isn't a widely recognized person, the question might not have an accurate answer based on public records. However, since I don't have access to real-time data or specific databases, I can't provide an exact birth year or age for them.\n",
      "\n",
      "Perhaps the user is confused about Ramcharan's role in history or literature and wants a general estimate of their presence in 2025. But without concrete information, it's tricky to give a precise answer. It might be best to suggest consulting official sources like birth records or historical databases for accurate data.\n",
      "\n",
      "In summary, I realize that without more context or access to specific data, I can't determine Ramcharan's age in 2025 accurately. I should reach out to official sources or reliable databases to get the most accurate information.\n",
      "</think>\n",
      "\n",
      "To determine Ramcharan's current age in 2025, we would need access to specific birth records or historical data for that person. Without this information, it is not possible to provide an accurate answer. It would be best to consult official birth records or reliable databases for precise details.\n",
      "\n",
      "**Final Answer:** The exact age of Ramcharan in 2025 cannot be determined without additional information from official sources.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Response:\n",
      "** The exact age of Ramcharan in 2025 cannot be determined without additional information from official sources.\n",
      "\n",
      "Token Usage:\n",
      "Total Tokens: input tokens + output tokens = 692\n",
      "Prompt Tokens: tokens in the input = 182\n",
      "Completion Tokens: tokens in the output = 510\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\"what is ramcharan's current age in the year 2025?\")\n",
    "    \n",
    "    # Print response content\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response)\n",
    "    \n",
    "    # Print token usage details\n",
    "    print(\"\\nToken Usage:\")\n",
    "    print(f\"Total Tokens: input tokens + output tokens = {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: tokens in the input = {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: tokens in the output = {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
