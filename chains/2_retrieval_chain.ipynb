{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load, Chunk, Embed, Store\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Load documents from a URL\n",
    "docs = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\").load()  \n",
    "\n",
    "# Split documents into chunks\n",
    "chunk = RecursiveCharacterTextSplitter()\n",
    "documents = chunk.split_documents(docs)              \n",
    "document_texts = [doc.page_content for doc in documents]  # Extract text\n",
    "\n",
    "# Embed document texts\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = embed.embed_documents(document_texts)  # Generate embeddings\n",
    "\n",
    "# Create and save FAISS vector store\n",
    "vector = FAISS.from_embeddings(list(zip(document_texts, embeddings)), embedding=embed)\n",
    "vector.save_local(\"C:/Users/admin/OneDrive/Desktop/RAG'S/vector db/Faiss-index/\")  # Store locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize Embeddings and Load FAISS Vector Store\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vector = FAISS.load_local(\"C:/Users/admin/OneDrive/Desktop/RAG'S/vector db/Faiss-index/\", embeddings=embed, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define the Context and Prompt Template\n",
    "context = \"\"\"LangSmith is a platform for developing, monitoring, and testing Large Language Model (LLM) applications.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Initialize LLM (Mixtral)\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "llm_mixtral = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Create Retrieval Chain (Combining Document Chain and Retriever)\n",
    "document_chain = create_stuff_documents_chain(llm_mixtral, prompt)\n",
    "retrieval_chain = create_retrieval_chain(vector.as_retriever(), document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a platform that can help with testing LLM (Language Learning Model) applications in several ways. First, it allows developers to create and use datasets as test cases, which can be uploaded in bulk or created on the fly. These test cases can be used to run custom evaluations and score test results. Additionally, LangSmith provides a comparison view for test runs, which allows users to view results for different configurations side-by-side and diagnose regressions in test scores across multiple revisions of an application. LangSmith also allows developers to attach feedback scores to logged traces, filter on traces with specific feedback tags and scores, and send runs to annotation queues for closer inspection and annotation by annotators. These features can help developers identify and root-cause issues, track regressions and improvements, and refine and improve the performance of their LLM applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?Explain in short.\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
