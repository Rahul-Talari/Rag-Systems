{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Retrieval Technique**                    | **Description**                                                                                  | **Key Attributes**                                                                                               |\n",
    "|--------------------------------------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| **1. Vector Store Retrieval**              | Uses embeddings to find the most similar documents by comparing the vector representations.     | - `search_type=\"similarity\"`: Finds similar documents. <br> - `k`: Number of documents to retrieve.            |\n",
    "| **2. MultiQueryRetriever**                 | Generates multiple query variations to enhance document retrieval.                               | - Uses LLM to generate query variations. <br> - Helps expand query scope and improve retrieval.               |\n",
    "| **3. MultiVectorRetriever**                | Stores multiple embeddings per document to improve search recall by considering different aspects. | - Uses multiple embeddings for each document. <br> - Improves recall by considering various document aspects.   |\n",
    "| **4. Max Marginal Relevance (MMR) Retriever** | Ensures diversity among retrieved results by balancing similarity and diversity.                 | - `search_type=\"mmr\"`: Ensures diversity. <br> - `lambda_mult`: Controls the balance between relevance and diversity. |\n",
    "| **5. Self Query Retriever (Metadata Filtering)** | Filters documents using metadata, like publication year.                                          | - Uses metadata to filter documents. <br> - `metadata_field_info`: Defines the metadata fields for filtering.   |\n",
    "| **6. BM25 (Lexical Search)**              | Uses traditional keyword-based search (term frequency, inverse document frequency).             | - Uses keyword matching. <br> - No embeddings involved.                                                        |\n",
    "| **7. Hybrid Search (BM25 + Vector Search)** | Combines keyword-based search (BM25) and embedding-based search for more robust results.        | - `EnsembleRetriever`: Combines multiple retrievers. <br> - Weights for each retriever.                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, MultiQueryRetriever, MultiVectorRetriever, SelfQueryRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# Step 1: Load webpage content and filter for relevant parts (title, headers, content).\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "docs = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ").load()\n",
    "\n",
    "# Step 2: Split the content into manageable chunks.\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "page_content = [doc.page_content for doc in docs]\n",
    "chunks = []\n",
    "for content in page_content:\n",
    "    chunks.extend(recursive_text_splitter.split_text(content))\n",
    "\n",
    "# Step 3: Generate embeddings using Ollama model.\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = embed.embed_documents(chunks)\n",
    "\n",
    "# Step 4: Create and store the FAISS vector store with the embeddings.\n",
    "vectorstore = FAISS.from_embeddings(list(zip(chunks, embeddings)), embedding=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groq LLM\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "llm= ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Search: 6\n"
     ]
    }
   ],
   "source": [
    "# --- Retrieval Techniques ---\n",
    "\n",
    "# 1. **Vector Store Retrieval (Similarity Search)**  \n",
    "retriever_similarity = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs_similarity = retriever_similarity.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(\"Similarity Search:\", len(retrieved_docs_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score Threshold: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. **Similarity Score Threshold Retrieval**  \n",
    "retriever_threshold = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8})\n",
    "retrieved_docs_threshold = retriever_threshold.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(\"Similarity Score Threshold:\", len(retrieved_docs_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiQueryRetriever: 5\n"
     ]
    }
   ],
   "source": [
    "# 2. **MultiQueryRetriever (Query Variations)**  \n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(vectorstore.as_retriever(), llm=llm)\n",
    "retrieved_docs_multi_query = multi_query_retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(\"MultiQueryRetriever:\", len(retrieved_docs_multi_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiVectorRetriever: 0\n"
     ]
    }
   ],
   "source": [
    "# 3. **MultiVectorRetriever (Multiple Embeddings per Document)**  \n",
    "from langchain_core.stores import InMemoryByteStore\n",
    "\n",
    "multi_vector_retriever = MultiVectorRetriever(vectorstore=vectorstore, byte_store=InMemoryByteStore())\n",
    "retrieved_docs_multi_vector = multi_vector_retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(\"MultiVectorRetriever:\", len(retrieved_docs_multi_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Marginal Relevance (MMR): 6\n"
     ]
    }
   ],
   "source": [
    "# 4. **Max Marginal Relevance (MMR)**  \n",
    "retriever_mmr = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6, 'lambda_mult': 0.25})\n",
    "retrieved_docs_mmr = retriever_mmr.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(\"Max Marginal Relevance (MMR):\", len(retrieved_docs_mmr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. **Self Query Retriever (Metadata Filtering)**  \n",
    "metadata_field_info = {\"date\": \"2023-06-23\", \"author\": \"Lilian Weng\"}  # Example metadata info\n",
    "self_query_retriever = SelfQueryRetriever.from_llm(vectorstore, llm, metadata_field_info=metadata_field_info,document_contents=page_content)\n",
    "retrieved_docs_self_query = self_query_retriever.invoke(\"What are the approaches to Task Decomposition published on 2023-06-23?\")\n",
    "print(\"Self Query Retriever:\", len(retrieved_docs_self_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Retrieval: 1\n"
     ]
    }
   ],
   "source": [
    "# 6. **BM25 (Lexical Search)**  \n",
    "from langchain.schema import Document\n",
    "\n",
    "documents = [Document(page_content=content) for content in page_content]\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)  # Using chunks instead of full documents\n",
    "retrieved_docs_bm25 = bm25_retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(\"BM25 Retrieval:\", len(retrieved_docs_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search (BM25 + Vector): 7\n"
     ]
    }
   ],
   "source": [
    "# 7. **Hybrid Search (BM25 + Vector Search)**  \n",
    "hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever_similarity], weights=[0.5, 0.5])\n",
    "retrieved_docs_hybrid = hybrid_retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(\"Hybrid Search (BM25 + Vector):\", len(retrieved_docs_hybrid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
