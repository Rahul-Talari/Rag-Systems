{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retrieval-Augmented Generation (RAG)**  \n",
    "\n",
    "**RAG is a technique for augmenting LLM knowledge with additional data.**  \n",
    "\n",
    "A RAG application has two main components:  \n",
    "\n",
    "1. **Indexing (Offline):** Ingests and indexes data for efficient retrieval.  \n",
    "2. **Retrieval & Generation (Online):** Processes user queries in real-time.  \n",
    "\n",
    "## **Workflow**  \n",
    "\n",
    "### **1. Indexing**  \n",
    "- **Load:** Data is ingested via **Document Loaders**.  \n",
    "- **Split:** Large documents are broken into smaller chunks using **Text Splitters**.  \n",
    "- **Store:** Chunks are indexed in a **VectorStore** with **Embeddings models**.  \n",
    "\n",
    "### **2. Retrieval & Generation**  \n",
    "- **Retrieve:** A **Retriever** fetches relevant chunks based on the user query.  \n",
    "- **Generate:** A **ChatModel / LLM** generates a response using retrieved data.  \n",
    "\n",
    "This ensures accurate, context-rich answers with optimized search efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Indexing: Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "docs = WebBaseLoader(web_paths=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\"],bs_kwargs={\"parse_only\": bs4_strainer}).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of documenth\",len(docs[0].page_content))\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500,chunk_overlap=200,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "\n",
    "# Extract the text content \n",
    "page_content = [doc.page_content for doc in docs]       \n",
    "\n",
    "# Step 3: Chunking\n",
    "chunks = []\n",
    "for content in page_content:\n",
    "    chunks.extend(recursive_text_splitter.split_text(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Print maximum and minimum chunk sizes\n",
    "chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "print('Maximum chunk size among all:', max(chunk_sizes))\n",
    "print('Minimum chunk size among all:', min(chunk_sizes))\n",
    "print('Total number of chunks is:', len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Embedding & Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = embed.embed_documents(chunks)  # Generate embeddings\n",
    "\n",
    "# Create and save FAISS vector store\n",
    "vectorstore = FAISS.from_embeddings(list(zip(chunks, embeddings)), embedding=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Chain (Sequential Execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Retrieve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(len(retrieved_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load the LLM model with specific parameters\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.8)\n",
    "\n",
    "# Define the prompt template for RAG (Retrieval-Augmented Generation)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context} \\n\"\n",
    "    \"Answer:\"\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain.stream(\"What is Agent ?Explain in short\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG Chain (Parallel Execution)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What is Task Decomposition?Explain in short\")\n",
    "print(len(retrieved_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Load the LLM model with specific parameters\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.8)\n",
    "\n",
    "# Define the prompt template for RAG (Retrieval-Augmented Generation)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context} \\n\"\n",
    "    \"Answer:\"\n",
    "])\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=rag_chain_with_source.invoke(\"What is Task Decomposition? Explain in short\")\n",
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
